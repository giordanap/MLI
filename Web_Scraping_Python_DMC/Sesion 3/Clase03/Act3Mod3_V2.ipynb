{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Act3Mod3.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PMYjyI0ODYhk",
        "colab_type": "text"
      },
      "source": [
        "## Actividad 3: Scrapear Frases v2.0\n",
        "\n",
        "En esta actividad 3 del módulo 3 scrapearemos un ecommerce extranjero.\n",
        "\n",
        "Es importante que nos guiemos de la documentación: https://docs.scrapy.org/en/latest/ para saber qué está ocurriendo en el código y aprender nuevas formas de ejecutar el scraping"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CHAapvpBDccg",
        "colab_type": "text"
      },
      "source": [
        "Con el código que vemos debajo podemos asegurarnos de instalar scrapy si es que no lo tenemos. Luego procedemos a importar el crawler que utilizaremos."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tEKnFMmeDDl0",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "try:\n",
        "    import scrapy\n",
        "except:\n",
        "    !pip install scrapy\n",
        "    import scrapy\n",
        "from scrapy.crawler import CrawlerProcess\n",
        "import re\n",
        "import logging"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VJLRT3dQDh0Y",
        "colab_type": "text"
      },
      "source": [
        "Diseñamos la estructura de nuestro Item."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ROZ1fI_xDj43",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class EcommerceItem(scrapy.Item):\n",
        "  title = scrapy.Field()\n",
        "  cat = scrapy.Field()\n",
        "  price = scrapy.Field()\n",
        "  descr = scrapy.Field()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iXLXcAGkDerv",
        "colab_type": "text"
      },
      "source": [
        "Diseñamos las funciones de nuestro Pipeline.\n",
        "* ```process_item``` es la función que se llama para procesar el objeto.\n",
        "* ```remove_tags``` es una función personalizada"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wnV9gDzeEgeO",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# se va a trabajar con este pipeline\n",
        "# \n",
        "class EcommercePipeline(object):\n",
        "    def process_item(self, item, spider):\n",
        "        return item\n",
        "    \n",
        "    def remove_tags(self, text):\n",
        "        html_tags = re.compile('<.*?>')\n",
        "        return re.sub(html_tags, '', text)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rE5Kuf5vI5Qh",
        "colab_type": "text"
      },
      "source": [
        "Una vez tenemos definida nuestra Pipeline, procedemos a definir nuestro propio Spider. Atención a las ```custom_settings```."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vPFJd2RcF3jW",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# le ponemos un nombre al spider\n",
        "class EcommerceSpider(scrapy.Spider):\n",
        "    name = 'EcommerceSpider'\n",
        "    allowed_domains = ['HarveyNorman.com.au']  # nuestro spider dice q se quede en esta web\n",
        "    start_urls = ['https://www.HarveyNorman.com.au/']\n",
        "\n",
        "    # log level para no llenarnos de mensajes\n",
        "    #  hemos definido un item hasta aqui\n",
        "    custom_settings = {\n",
        "        'LOG_LEVEL': logging.WARNING, # Con esto reducimos el número de mensajes que recibimos\n",
        "        'ITEM_PIPELINES': {'__main__.EcommercePipeline': 1}, # Ejecuta nuestra Pipeline\n",
        "        'FEED_FORMAT':'csv',                                 # Almacenamos la información de nuestra Pipeline\n",
        "        'FEED_URI': 'ecommerce.csv'\n",
        "    }\n",
        "# q es init? me permite añadir funcionalidad de constructor, se llena todos los valores de abajo\n",
        "# son los expath a distintos elementos\n",
        "    def __init__(self):\n",
        "        #self.getAllCategoriesXpath = \"//div[@id='wrapper']/div[1]/div[1]/div[1]/div[1]/div[contains(@class, 'col-md-3')]/ul/li/a/@href\"\n",
        "        #self.getAllSubCategoriesXpath = \"//*[@id='content']/div[2]/div[1]/div/div[2]/div/div/div/div[2]/div/a/@href\"\n",
        "        self.getAllItemsXpath = \"//*[@id='category-grid']/div/div/div[3]/a/@href\" # es el selector q me traes todas las etiquetas de los productos de la portada, me pide el hipervinculo del producto\n",
        "        #self.getAllItemsXpath = \"//*[@id='category-grid']/div/div/div[@class='info']/a/@href\" # por si cambia el nombre d euna clase\n",
        "        self.titleXpath  = \"//*[@id='overview']/div[1]/h1/span[1]/text()\"\n",
        "        self.categoryXpath = \"//*[@id='breadcrumbs']/li/a/text()\"\n",
        "        self.priceXpath = \"//div[contains(@class, 'product-view-sales')]//span[@class='price']/text()\"\n",
        "        self.descriptionXpath = \"//*[@id='tab-content-product-description']/div/div[contains(@class,'description')][1]/p//text()\"\n",
        "\n",
        " # porque hay 2 funciones de parse\n",
        " # va scrapear mi portada y va acalcular el hiperviculo\n",
        " # el segundo    \n",
        "    def parse(self, response):\n",
        "        for href in response.xpath(self.getAllItemsXpath): # q estrae? getxpath habia link en cada uno\n",
        "            url = response.urljoin(href.extract()) # urljoin nos permite trabajar con la direccion completa\n",
        "            yield scrapy.Request(url,callback=self.parse_main_item, dont_filter=True) # \n",
        "            # entrada x entrada, url=seria la web especifica de un producto\n",
        "            # callback=yo hago este request sobre este url\n",
        "            # donfilter = para q me salga todos los resultados vacios o no vacios\n",
        "\n",
        "# primero inicializar un item\n",
        "# y luego extraemos con nuestras expath\n",
        "# es return ya q no esta iterando, se esta extrayendo de una pagina\n",
        "    def parse_main_item(self,response):\n",
        "        item = EcommerceItem()\n",
        "\n",
        "        title = response.xpath(self.titleXpath).extract()\n",
        "        category = response.xpath(self.categoryXpath).extract()\n",
        "        price = response.xpath(self.priceXpath).extract()\n",
        "        description = response.xpath(self.descriptionXpath).extract()\n",
        "\n",
        "        item['title'] = title\n",
        "        item['cat'] = category\n",
        "        item['price'] = price\n",
        "        item['descr'] = description\n",
        "        return item"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8q2cBBtHHRyM",
        "colab_type": "code",
        "outputId": "0b0d1db9-c522-4e96-a2f3-60846abcade3",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 88
        }
      },
      "source": [
        "process = CrawlerProcess()\n",
        "\n",
        "process.crawl(EcommerceSpider)\n",
        "process.start()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "2019-11-18 20:31:57 [scrapy.utils.log] INFO: Scrapy 1.8.0 started (bot: scrapybot)\n",
            "2019-11-18 20:31:57 [scrapy.utils.log] INFO: Versions: lxml 4.2.6.0, libxml2 2.9.8, cssselect 1.1.0, parsel 1.5.2, w3lib 1.21.0, Twisted 19.10.0, Python 3.6.8 (default, Oct  7 2019, 12:59:55) - [GCC 8.3.0], pyOpenSSL 19.1.0 (OpenSSL 1.1.1d  10 Sep 2019), cryptography 2.8, Platform Linux-4.14.137+-x86_64-with-Ubuntu-18.04-bionic\n",
            "2019-11-18 20:31:57 [scrapy.crawler] INFO: Overridden settings: {'FEED_FORMAT': 'csv', 'FEED_URI': 'ecommerce.csv', 'LOG_LEVEL': 30}\n"
          ],
          "name": "stderr"
        }
      ]
    }
  ]
}